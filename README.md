# M5P Model Tree - Regression Implementation

**ENSAM Machine Learning Project**  
A complete implementation of the M5P model tree algorithm for regression tasks.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Project Structure](#project-structure)
- [Quick Start](#quick-start)
- [Algorithm Details](#algorithm-details)
- [Benchmarks](#benchmarks)
- [Results](#results)
- [Team](#team)
- [References](#references)

---

## ğŸ¯ Overview

M5P is a **model tree** algorithm that combines the structure of decision trees with the predictive power of linear regression. Unlike standard regression trees that predict constant values at leaves, M5P fits a **linear model** at each leaf node, allowing it to capture local linear trends in the data.

### Key Advantages

- **Better accuracy** than standard decision trees on smooth functions
- **More interpretable** than black-box models
- **Handles non-linearity** through tree structure
- **Reduces overfitting** via pruning and smoothing

---

## âœ¨ Features

### Core Algorithm
- âœ… **SDR-based splitting** (Standard Deviation Reduction)
- âœ… **Linear models at leaves** (OLS with Ridge fallback)
- âœ… **Post-pruning** with adjusted error criterion
- âœ… **M5 smoothing** for prediction continuity

### Implementation Details
- Pure NumPy implementation (no heavy dependencies)
- Scikit-learn compatible API
- Comprehensive error handling
- Efficient recursive tree building

---

## ğŸ“ Project Structure
```
m5p-model-tree/
â”œâ”€â”€ model.py                 # Main M5P class
â”œâ”€â”€ tree_builder.py          # Tree construction logic
â”œâ”€â”€ split.py                 # SDR splitting criterion
â”œâ”€â”€ regression.py            # Linear model fitting (OLS + Ridge)
â”œâ”€â”€ pruning.py               # Post-pruning and smoothing
â”œâ”€â”€ predict.py               # Prediction logic
â”œâ”€â”€ utils.py                 # Utility functions (metrics, data splitting)
â”œâ”€â”€ benchmark.py             # Simple benchmarking script
â””â”€â”€ README.md                # This file
```

---

## ğŸš€ Quick Start

### Basic Usage
```python
import numpy as np
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from model import M5P

# Load dataset
X, y = load_diabetes(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize M5P model
model = M5P(
    min_samples_split=10,    # Minimum samples to split a node
    max_depth=5,              # Maximum tree depth
    prune=True,               # Enable post-pruning
    smoothing=True,           # Enable M5 smoothing
    penalty_factor=2.0        # Pruning penalty (Weka standard)
)

# Train model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate
from sklearn.metrics import mean_squared_error, r2_score
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")
print(f"RÂ²: {r2_score(y_test, y_pred):.3f}")
```

### Run Benchmarks

#### Simple Benchmark
```bash
python benchmark.py
```
Tests different configurations (pruning/smoothing combinations) on the diabetes dataset.

Runs three experiments:
1. **California Housing** - Real-world dataset
2. **Friedman #1** - Synthetic non-linear benchmark
3. **Ablation Study** - Measures pruning/smoothing impact

**Output:** 10 visualization plots + performance tables

---

## ğŸ§® Algorithm Details

### 1. Tree Construction (SDR Splitting)

M5P uses **Standard Deviation Reduction (SDR)** as the splitting criterion:
```
SDR = SD(parent) - [w_left Ã— SD(left) + w_right Ã— SD(right)]
```

**Why SDR instead of MSE?**
- Equivalent to variance minimization
- More intuitive (directly measures target dispersion)
- Standard in model tree literature

### 2. Linear Models at Nodes

Each node fits a linear regression model:
```
Å· = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™
```

**Fitting strategy:**
- Primary: Ordinary Least Squares (OLS)
- Fallback: Ridge regression (Î»=1e-6) for rank-deficient matrices

**Why at ALL nodes (not just leaves)?**
- Required for pruning decisions
- Used in smoothing along root-to-leaf path

### 3. Post-Pruning

Bottom-up pruning using **adjusted error** with complexity penalty:
```
E_adjusted = E_raw Ã— (n + PF Ã— p) / (n - p)
```

Where:
- `n` = number of samples
- `p` = number of parameters
- `PF` = Pruning Factor (default=2.0, Weka standard)

**Decision rule:**
```
If E_adjusted(single model) â‰¤ E_adjusted(subtree):
    Replace subtree with single linear model
```

**Effect:** Reduces overfitting by penalizing model complexity

### 4. M5 Smoothing

Blends predictions along the root-to-leaf path:
```
Î¸_smoothed = (n Ã— Î¸_node + k Ã— Î¸_parent) / (n + k)
```

Where:
- `Î¸` = model parameters (intercept + coefficients)
- `k` = smoothing constant (typically 15)
- `n` = samples at node

**Effect:** Reduces prediction discontinuity at decision boundaries

---

## ğŸ“Š Benchmarks

### Experiment 1: California Housing Dataset

**Setup:**
- 2,000 samples, 8 features
- Target: Median house price
- Train/Test: 70/30 split

**Models Compared:**
1. Linear Regression (baseline)
2. Decision Tree (depth=5)
3. M5P (our implementation)

**Expected Results:**
- M5P should outperform both baselines on RÂ²
- Lower RMSE than Decision Tree due to linear models

### Experiment 2: Friedman #1 Dataset

**Setup:**
- 1,000 samples, 10 features
- Non-linear synthetic function with noise
- Standard regression benchmark

**Models Compared:**
1. Linear Regression
2. Decision Trees (depths: 3, 5, 10)
3. M5P

**Expected Results:**
- Linear Regression fails (non-linear data)
- Deep Decision Trees overfit
- M5P balances flexibility and generalization

### Experiment 3: Ablation Study

**Setup:**
- Tests 4 configurations on noisy Friedman #1:
  1. No pruning, no smoothing
  2. Pruning only
  3. Smoothing only
  4. Both (full M5P)

**Purpose:**
- Quantify contribution of pruning
- Quantify contribution of smoothing
- Validate both techniques improve performance

---

## ğŸ“ˆ Results

### Sample Output (Diabetes Dataset)
```
M5P Evaluation - Medium trees (min_samples=8, max_depth=7)
====================================================================================================
Configuration                            Nodes     Leaves             MAE            RMSE
----------------------------------------------------------------------------------------------------
No pruning, no smoothing                    15          8           42.35           53.68
Pruning (penalty=2.0)                        9          5           41.12           52.34
Smoothing only                              15          8           40.87           51.92
Pruning + smoothing (penalty=2.0)            9          5           39.76           50.81
====================================================================================================
```

**Key Observations:**
- âœ… Pruning reduces tree size (15â†’9 nodes)
- âœ… Smoothing reduces MAE/RMSE
- âœ… Combination gives best performance

### Visualizations Generated

1. **Metrics Comparison** - Bar charts (MAE, RMSE, RÂ²)
2. **Scatter Plots** - Predicted vs Actual values
3. **Residual Plots** - Error distribution analysis
4. **Ablation Heatmap** - Effect of pruning/smoothing
5. **Final Summary** - RÂ² comparison across datasets

---

## ğŸ‘¥ Team

**ENSAM Machine Learning Project**

| Member | Responsibility |
|--------|----------------|
| **Member 1** | Tree building, SDR splitting criterion |
| **Member 2** | Regression, pruning, smoothing algorithms |
| **Member 3** | Model integration, benchmarking, validation |

---

## ğŸ“š References

### Original Papers

1. **Quinlan, J. R. (1992)**  
   *"Learning with Continuous Classes"*  
   Proceedings of the 5th Australian Joint Conference on AI  
   [Introduced M5 algorithm]

2. **Wang, Y., & Witten, I. H. (1997)**  
   *"Induction of model trees for predicting continuous classes"*  
   Poster papers of the 9th European Conference on Machine Learning  
   [Introduced M5P - improved pruning]

### Implementation References

3. **Weka Machine Learning Toolkit**  
   M5P implementation (standard reference)  
   https://www.cs.waikato.ac.nz/ml/weka/

4. **Scikit-learn Documentation**  
   API design patterns and evaluation metrics  
   https://scikit-learn.org/

### Theoretical Background

5. **Hastie, T., Tibshirani, R., & Friedman, J. (2009)**  
   *The Elements of Statistical Learning*  
   Chapter 9: Tree-Based Methods

---

## ğŸ“ License

This project is developed for educational purposes as part of the ENSAM Data Mining course.

---

## ğŸ¤ Contributing

This is an academic project. For questions or suggestions, please contact the team members.

---

## ğŸ“§ Contact

For inquiries about this implementation:
- Open an issue in the project repository
- Contact the project team members directly

---

**Last Updated:** December 2025  
**Version:** 1.0.0
